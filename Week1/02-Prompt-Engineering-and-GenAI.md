# Week 1: Prompt Engineering and Generative AI

## Key Terminology Recap
- **Prompt**: Text that you feed into the model
- **Inference**: Act of generating text
- **Completion**: Output text generated by the model
- **Context Window**: Full amount of text/memory available for the prompt

## Prompt Engineering
- **Definition**: Work to develop and improve prompts to get desired model behavior
- **Challenge**: Models often don't produce desired outcomes on first try
- **Solution**: Revise language and structure of prompts iteratively
- **Key Strategy**: Include examples of the task within the prompt

## In-Context Learning
**Definition**: Including examples or additional data in the prompt to help LLMs learn more about the task

### Types of Inference

#### 1. Zero-Shot Inference
- **Method**: Include input data within prompt without examples
- **Format**: Instruction + Context + Output request
- **Example**: "Classify this review: [review text] Sentiment:"
- **Performance**: 
  - Large models (like GPT-3/4): Surprisingly good at zero-shot tasks
  - Smaller models (like GPT-2): Often struggle and don't follow instructions

#### 2. One-Shot Inference
- **Method**: Include a single completed example in the prompt
- **Format**: Instruction + Example + Actual task
- **Example**:
  ```
  Classify this review:
  "I loved this movie" - Positive
  
  Classify this review:
  "[actual review text]" - 
  ```
- **Benefit**: Helps smaller models understand task format and requirements

#### 3. Few-Shot Inference
- **Method**: Include multiple examples with different output classes
- **Purpose**: When single example isn't enough for model understanding
- **Strategy**: Mix examples with different outcomes (positive/negative, etc.)
- **Result**: Better task comprehension and performance

## Model Scale and Performance Relationship

### Large Models
- **Capability**: Good at zero-shot inference
- **Performance**: Can infer and complete tasks not specifically trained for
- **Flexibility**: Able to perform multiple tasks well

### Smaller Models
- **Limitation**: Generally good at small number of tasks
- **Requirement**: Need examples (one-shot or few-shot) for better performance
- **Focus**: Typically perform well on tasks similar to training data

## Important Considerations

### Context Window Limitations
- **Constraint**: Limited amount of in-context learning possible
- **Rule of Thumb**: If 5-6 examples don't improve performance, consider fine-tuning instead
- **Alternative**: Fine-tuning (additional training with new data) - covered in Week 2

### Model Selection
- **Process**: May need to try multiple models to find right fit for use case
- **Factors**: Task complexity, required performance, available examples

### Configuration Settings
- **Purpose**: Influence structure and style of model completions
- **Flexibility**: Various settings available to experiment with
- **Optimization**: Fine-tune settings based on specific requirements

## Key Takeaways
1. **Prompt engineering is iterative** - expect to revise prompts multiple times
2. **Examples improve performance** - especially for smaller models
3. **Model size matters** - larger models are more capable at zero-shot tasks
4. **Context window is finite** - balance between examples and input length
5. **Fine-tuning is alternative** - when in-context learning isn't sufficient
6. **Model selection is crucial** - different models for different use cases

## Inference Configuration Parameters

### Overview
- **Purpose**: Control how models make final decisions about next-word generation
- **Nature**: Different from training parameters; invoked at inference time
- **Available in**: LLM playgrounds (Hugging Face, AWS, etc.)
- **Control**: Maximum tokens, creativity level, output style

### Key Parameters

#### 1. Max New Tokens
- **Function**: Limits the number of tokens the model will generate
- **Behavior**: Puts a cap on selection process iterations
- **Important**: It's a maximum, not a fixed number
- **Stop Conditions**: Generation may end early due to end-of-sequence tokens
- **Examples**: Setting to 100, 150, or 200 tokens

#### 2. Decoding Methods

##### Greedy Decoding (Default)
- **Method**: Always choose word with highest probability
- **Pros**: Simple, reliable for short generations
- **Cons**: Susceptible to repeated words/sequences
- **Use Case**: When consistency is more important than creativity

##### Random Sampling
- **Method**: Choose words randomly using probability distribution as weights
- **Example**: Word with 0.02 probability = 2% chance of selection
- **Pros**: Reduces repetition, increases variability
- **Cons**: May produce overly creative or nonsensical output
- **Implementation**: May require enabling (e.g., `do_sample=True` in Hugging Face)

#### 3. Top-k Sampling
- **Method**: Limit random sampling to k tokens with highest probability
- **Process**: 
  1. Select top k most probable tokens
  2. Use probability weighting among these k options
  3. Choose randomly from this restricted set
- **Benefit**: Maintains randomness while preventing highly improbable words
- **Result**: More reasonable and sensible text generation
- **Example**: k=3 restricts choice to 3 most likely words

#### 4. Top-p Sampling (Nucleus Sampling)
- **Method**: Limit sampling to predictions whose combined probabilities â‰¤ p
- **Process**: 
  1. Add probabilities of most likely words until sum reaches p
  2. Choose randomly from this subset using probability weighting
- **Example**: p=0.3 might include words with probabilities 0.2 + 0.1 = 0.3
- **Difference from Top-k**: Specifies total probability rather than number of tokens
- **Flexibility**: Dynamic number of candidate words based on probability distribution

#### 5. Temperature
- **Function**: Controls randomness by scaling the probability distribution
- **Impact**: Actually alters the predictions the model makes
- **Application**: Applied in final softmax layer before sampling

##### Low Temperature (< 1.0)
- **Effect**: More strongly peaked probability distribution
- **Result**: 
  - Probability concentrated in fewer words
  - Less random output
  - Follows most likely word sequences from training
  - More predictable and conservative text

##### High Temperature (> 1.0)
- **Effect**: Broader, flatter probability distribution
- **Result**: 
  - Probability more evenly spread across tokens
  - Higher degree of randomness
  - More creative and variable output
  - Less predictable text

##### Temperature = 1.0
- **Effect**: Default softmax function, unaltered probability distribution
- **Use**: Baseline setting for comparison

### Parameter Combinations
- **Best Practice**: Combine parameters for optimal results
- **Example**: Use top-k or top-p with appropriate temperature
- **Experimentation**: Test different combinations for specific use cases
- **Balance**: Find sweet spot between creativity and coherence

### Practical Applications
- **Creative Writing**: Higher temperature, top-p sampling
- **Technical Documentation**: Lower temperature, greedy decoding
- **Conversational AI**: Moderate temperature with top-k sampling
- **Code Generation**: Lower temperature for accuracy

## Summary: Complete LLM Foundation
- **Transformer Architecture**: Understanding the underlying model structure
- **Prompt Engineering**: Crafting effective prompts with examples
- **Inference Configuration**: Fine-tuning output behavior and creativity
- **Next Step**: Building and launching LLM-powered applications
